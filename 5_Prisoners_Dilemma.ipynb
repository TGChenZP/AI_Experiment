{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Investigation: Using \"Prisoners Dilemma\"to experiment with using finding Equilibrium of simple game theory games #\n",
    "## AI Investigation Part 5 ##\n",
    "\n",
    "#### *Goal: test basic theory of using simulation to determine equilibrium of game theory games* ####\n",
    "#### *Also experiments a new way to present the winloss function* ####\n",
    "\n",
    "Creator: `Lang (Ron) Chen` 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import statistics as s\n",
    "import scipy.stats\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Game\n",
    "\n",
    "If both confesses, then each get 10 years in prison;\n",
    "\n",
    "If A confesses and B does not, then A gets 1 year and B gets 25 years;\n",
    "\n",
    "Vice versa if B confesses and A does not;\n",
    "\n",
    "If both do not confess then they each get 3 years;\n",
    "\n",
    "<img src = \"./images/PrisonersDilemma.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOICE = {'Confess': 0, 'Not Confess':1}\n",
    "OUTCOME = [[-10, -1], \n",
    "           [-25, -3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is an experiment on upgrading the winloss function from using a host of if-else to a more lightweight structure. However this wouldn't work as well for games with more than two players, or if the CHOICEs are not discrete.*\n",
    "\n",
    "*Also if two players have different payoffs then need two different OUTCOMEs, but this would still be more concise than writing two distinct winloss functions with massive if-else overheads*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winloss(player1, player2):\n",
    "    return OUTCOME[CHOICE[player1]][CHOICE[player2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(choice, CHOICE):\n",
    "    if \"Inconclusive\" in choice:\n",
    "        return 'Inconclusive'\n",
    "    if choice not in CHOICE:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNS = 1000000\n",
    "\n",
    "data1 = defaultdict(list)\n",
    "data2 = defaultdict(list) # Using defaultdict makes the code more adaptable to different games - and the validation function would ensure that the final choice is valid. However it is up to the user to double check as we are not hardcoding the choices\n",
    "\n",
    "sample = list()\n",
    "victory = list()\n",
    "\n",
    "choicekeys = list(CHOICE.keys())\n",
    "for i in range(RUNS):\n",
    "    obs1 = random.sample(choicekeys, 1)[0]\n",
    "    obs2 = random.sample(choicekeys, 1)[0]\n",
    "    \n",
    "    data1[obs1].append(winloss(obs1, obs2))\n",
    "    data2[obs2].append(winloss(obs2, obs1))\n",
    "    \n",
    "# This time we record the data for both players because we are interested in each's dominant strategy,  which will help us determine the equilibrium\n",
    "# i.e. we are not as interested in, or equally interested in each's dominant strategy as well as the final equilibrium state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm for final choice**\n",
    "\n",
    "First: Manipulate data so that it is in a dictionary and the dictionary value is [mean, stdev, length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Confess': [-5.509312564171198, 4.499994858484077, 500614],\n",
       " 'Not Confess': [-14.01057298362389, 11.000005932262946, 499386]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats1 = dict()\n",
    "for choice in CHOICE:\n",
    "    tmp = list()\n",
    "    tmp.append(s.mean(data1[choice]))\n",
    "    tmp.append(s.stdev(data1[choice]))\n",
    "    tmp.append(len(data1[choice]))\n",
    "    \n",
    "    stats1[choice] = tmp\n",
    "    \n",
    "stats1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Confess': [-5.5080158479744705, 4.49999735386907, 500758],\n",
       " 'Not Confess': [-14.007403223286502, 11.000008525455318, 499242]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats2 = dict()\n",
    "for choice in CHOICE:\n",
    "    tmp = list()\n",
    "    tmp.append(s.mean(data2[choice]))\n",
    "    tmp.append(s.stdev(data2[choice]))\n",
    "    tmp.append(len(data2[choice]))\n",
    "    \n",
    "    stats2[choice] = tmp\n",
    "    \n",
    "stats2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_choice(stats):\n",
    "    \n",
    "    stats.sort(key = lambda x:x[1][0], reverse = True)\n",
    "\n",
    "    # Then tests whether the second, third and so forth values contain 0 within their joint Confidence Intervals\n",
    "    inconclusive_list = [stats[0][0]]\n",
    "    inconclusive = False\n",
    "    for i in range(1, len(stats)):\n",
    "        if in_range(construct_CI(stats[0], stats[i])):\n",
    "            inconclusive_list.append(stats[i][0])\n",
    "            inconclusive = True\n",
    "        else: # Because all values are sorted, if the current choice's joint Confidence Interval with the first ranked does not contain 0, then all the others wouldn't \n",
    "            break\n",
    "            \n",
    "    if inconclusive: # If inconclusive, return statement with the list of 'drawed' choices\n",
    "        return f'Inconclusive: the following came to a draw {inconclusive_list}'\n",
    "    \n",
    "    return stats[0][0] # Else, return the dominant strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_CI(stat1, stat2):\n",
    "    \"\"\" Uses Welch's approximation to construct a joint CI of two means, unknown population variance and assuming different variance \"\"\"\n",
    "    \n",
    "    xbar1 = stat1[1][0]\n",
    "    xbar2 = stat2[1][0]\n",
    "    s1 = stat1[1][1]\n",
    "    s2 = stat2[1][1]\n",
    "    n = stat1[1][2]\n",
    "    m = stat2[1][2]\n",
    "    \n",
    "    r = (s1**2 /n + s2**2 /m)**2 /(s1**4 /(n**2 * (n-1)) + s2**4 /(m**2 * (m-1)))\n",
    "    \n",
    "    q = scipy.stats.t.ppf(0.95, df = r)\n",
    "    \n",
    "    poolsd = np.sqrt(s1**2 /n + s2**2 /m)\n",
    "    \n",
    "    CI = (xbar1 - xbar2 - q * poolsd, xbar1 - xbar2 + q * poolsd)\n",
    "    \n",
    "    print(f'{stat1[0]} {stat2[0]}: {CI}')\n",
    "    print('\\n')\n",
    "    \n",
    "    return CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_range(CI):\n",
    "    \"\"\" Helper function to check whether 0 is within the Confidence Interval \"\"\"\n",
    "    \n",
    "    if CI[0] <= 0 and CI[1] >= 0:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Confess': [-5.509312564171198, 4.499994858484077, 500614],\n",
       " 'Not Confess': [-14.01057298362389, 11.000005932262946, 499386]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confess Not Confess: (8.473601980530878, 8.528918858374505)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Confess'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = final_choice(list(stats1.items()))\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Confess': [-5.5080158479744705, 4.49999735386907, 500758],\n",
       " 'Not Confess': [-14.007403223286502, 11.000008525455318, 499242]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confess Not Confess: (8.47172607945118, 8.527048671172885)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Confess'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = final_choice(list(stats2.items()))\n",
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(result1, CHOICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(result2, CHOICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Emperical Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-10, -10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "victory1 = list()\n",
    "victory2 = list()\n",
    "for i in range(RUNS):\n",
    "    victory1.append(winloss(result1, result2))\n",
    "    victory2.append(winloss(result2, result1))\n",
    "\n",
    "Equilibrium = (s.mean(victory1), s.mean(victory2))\n",
    "Equilibrium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the equilibrium of this game is (-10, -10), where both convicts' dominant strategy is to confess "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
